# ğŸŒ Semantic Segmentation â€“ Projeto de DetecÃ§Ã£o em Imagens de SatÃ©lite

Este projeto implementa um pipeline completo para **segmentaÃ§Ã£o semÃ¢ntica** utilizando imagens de satÃ©lite (Sentinel-2).  
O objetivo Ã© identificar e separar diferentes regiÃµes de interesse em imagens geoespaciais, aplicando **Deep Learning** (U-Net) e ferramentas de visualizaÃ§Ã£o e monitoramento de mÃ©tricas.

---

## ğŸ“‚ Estrutura do Projeto

```
initial-challenge-semantic-segmentation/
â”‚
â”œâ”€ data/                     # Pasta de dados (imagens .tif, mÃ¡scaras, etc.)
â”œâ”€ checkpoints/               # Modelos treinados (.pth)
â”œâ”€ src/                       # CÃ³digo-fonte principal
â”‚   â”œâ”€ train.py               # Treinamento do modelo
â”‚   â”œâ”€ infer.py               # InferÃªncia e geraÃ§Ã£o de previsÃµes
â”‚   â”œâ”€ explain.py             # InterpretaÃ§Ã£o com Grad-CAM
â”‚   â”œâ”€ metrics.py             # FunÃ§Ãµes de avaliaÃ§Ã£o
â”‚   â”œâ”€ utils.py               # FunÃ§Ãµes auxiliares (prÃ©-processamento, etc.)
â”‚   â””â”€ models.py              # DefiniÃ§Ã£o da arquitetura U-Net
â”œâ”€ requirements.txt           # DependÃªncias do projeto
â””â”€ README.md                  # Este arquivo
```

---

## ğŸš€ Fluxo do Projeto

### 1ï¸âƒ£ **PrÃ©-processamento dos Dados**
As imagens de satÃ©lite (GeoTIFF) sÃ£o carregadas com **Rasterio**, normalizadas e organizadas em tensores PyTorch.  
Isso garante que o modelo receba dados padronizados para aprendizado.

### 2ï¸âƒ£ **Modelo â€“ U-Net**
- A arquitetura escolhida Ã© a **U-Net**, amplamente usada em segmentaÃ§Ã£o de imagens.  
- Ela possui um **encoder** (extrai caracterÃ­sticas) e um **decoder** (reconstrÃ³i a mÃ¡scara pixel a pixel).

O modelo Ã© definido em `models.py` e pode ser facilmente ajustado para outros datasets ou camadas.

### 3ï¸âƒ£ **Treinamento (`train.py`)**
- O treinamento utiliza **PyTorch** para otimizaÃ§Ã£o.  
- As mÃ©tricas de desempenho, como **IoU** e **F1-Score**, sÃ£o calculadas em cada Ã©poca.  
- A integraÃ§Ã£o com **Weights & Biases (wandb)** registra:
  - GrÃ¡ficos de perda (loss)
  - MÃ©tricas de validaÃ§Ã£o
  - Checkpoints do modelo

Isso permite acompanhar o aprendizado em tempo real e comparar diferentes execuÃ§Ãµes.

### 4ï¸âƒ£ **InferÃªncia (`infer.py`)**
- Carrega o modelo treinado (checkpoint `.pth`) e aplica em novas imagens.
- Gera mÃ¡scaras segmentadas salvas como imagens ou arquivos GeoTIFF.

### 5ï¸âƒ£ **InterpretaÃ§Ã£o com Grad-CAM (`explain.py`)**
Para entender **quais regiÃµes da imagem influenciam mais as previsÃµes**, usamos o **Grad-CAM**:
- Destaca Ã¡reas crÃ­ticas para a decisÃ£o da rede.
- Gera um overlay (heatmap) sobre a imagem original.
- Facilita a anÃ¡lise e explicaÃ§Ã£o do modelo, aumentando a transparÃªncia.

---

## ğŸ› ï¸ Principais Tecnologias
- **PyTorch** â€“ Treinamento e inferÃªncia da rede neural.
- **Rasterio** â€“ Leitura de imagens geoespaciais.
- **Weights & Biases (wandb)** â€“ Monitoramento de mÃ©tricas.
- **Grad-CAM** â€“ InterpretaÃ§Ã£o visual.

---

## ğŸ’¡ PossÃ­veis ExtensÃµes
- **Augmentations** (aumentos de dados) para melhorar a robustez.
- Testes com outras arquiteturas (DeepLab, UNet++).  
- GeraÃ§Ã£o de dashboards interativos com **Streamlit** ou **Dash**.

---

## âš¡ Como Rodar

1ï¸âƒ£ Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

2ï¸âƒ£ Treine o modelo:
```bash
python src/train.py
```

3ï¸âƒ£ FaÃ§a a inferÃªncia:
```bash
python src/infer.py
```

4ï¸âƒ£ Gere visualizaÃ§Ãµes Grad-CAM:
```bash
python src/explain.py
```

---

## ğŸ“Š Resultado Esperado
Ao final, vocÃª terÃ¡:
- **Modelo treinado** para segmentaÃ§Ã£o.
- **MÃ¡scaras preditas** em novas imagens.
- **VisualizaÃ§Ãµes interpretÃ¡veis** (Grad-CAM).
- HistÃ³rico de treinamento registrado no **wandb**.

---

ğŸ” **Resumo RÃ¡pido**:  
Este projeto demonstra um pipeline moderno de **Deep Learning aplicado a geoinformaÃ§Ã£o**, com:
- Treinamento de uma rede U-Net em imagens de satÃ©lite.
- AvaliaÃ§Ã£o quantitativa (mÃ©tricas).
- VisualizaÃ§Ã£o interpretÃ¡vel (Grad-CAM).
- Registro de experimentos em tempo real.

